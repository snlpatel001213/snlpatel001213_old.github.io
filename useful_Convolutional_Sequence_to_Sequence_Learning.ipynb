{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "useful-Convolutional Sequence to Sequence Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "PlVJ5YHjfeEc",
        "colab_type": "code",
        "outputId": "58240b73-7f23-4cd9-eeb9-a031554a4d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "\n",
            "    You can now load the model via spacy.load('de')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k-JQLV5qd7w4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import nltk\n",
        "\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import spacy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zM0Maijqd7xA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mqcd2cQ_d7xF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UU5J3xRCd7xK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ik-3-Y5Id7xQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
        "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKjOEiFRd7xV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6mrxlQhUd7xa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Kqv3cSId7xg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gr1PiJHld7xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size=BATCH_SIZE,\n",
        "     device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziU4CYRad7xs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd (for now)\"\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, 2*hid_dim, kernel_size, padding=(kernel_size-1)//2)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src sent len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(src)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = pos_embedded= [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, src sent len]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(self.dropout(conv_input))\n",
        "\n",
        "            #conved = [batch size, 2*hid dim, src sent len]\n",
        "\n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim=1)\n",
        "\n",
        "            #conved = [batch size, hid dim, src sent len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "\n",
        "            #conved = [batch size, hid dim, src sent len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
        "        combined = (conved + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        return conved, combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nfRmgn_Vd7xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dropout = dropout\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear(emb_dim, output_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(hid_dim, 2*hid_dim, kernel_size)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #embedded = [batch size, trg sent len, emb dim]\n",
        "        #conved = [batch size, hid dim, trg sent len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved_emb = [batch size, trg sent len, emb dim]\n",
        "        \n",
        "        combined = (embedded + conved_emb) * self.scale\n",
        "        \n",
        "        #combined = [batch size, trg sent len, emb dim]\n",
        "                \n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
        "        \n",
        "        #energy = [batch size, trg sent len, src sent len]\n",
        "        \n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        #attention = [batch size, trg sent len, src sent len]\n",
        "            \n",
        "        attended_encoding = torch.matmul(attention, (encoder_conved + encoder_combined))\n",
        "        \n",
        "        #attended_encoding = [batch size, trg sent len, emd dim]\n",
        "        \n",
        "        #convert from emb dim -> hid dim\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
        "        \n",
        "        #attended_combined = [batch size, hid dim, trg sent len]\n",
        "        \n",
        "        return attention, attended_combined\n",
        "        \n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #trg = [batch size, trg sent len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src sent len, emb dim]\n",
        "                \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(device)\n",
        "        \n",
        "        #pos = [batch size, trg sent len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(trg)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = [batch size, trg sent len, emb dim]\n",
        "        #pos_embedded = [batch size, trg sent len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, trg sent len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, trg sent len]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #apply dropout\n",
        "            conv_input = self.dropout(conv_input)\n",
        "        \n",
        "            #need to pad so decoder can't \"cheat\"\n",
        "            padding = torch.zeros(conv_input.shape[0], conv_input.shape[1], self.kernel_size-1).fill_(self.pad_idx).to(device)\n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim=2)\n",
        "        \n",
        "            #padded_conv_input = [batch size, hid dim, trg sent len + kernel size - 1]\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(padded_conv_input)\n",
        "\n",
        "            #conved = [batch size, 2*hid dim, trg sent len]\n",
        "            \n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim=1)\n",
        "\n",
        "            #conved = [batch size, hid dim, trg sent len]\n",
        "            \n",
        "            attention, conved = self.calculate_attention(embedded, conved, encoder_conved, encoder_combined)\n",
        "            \n",
        "            #attention = [batch size, trg sent len, src sent len]\n",
        "            #conved = [batch size, hid dim, trg sent len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "            \n",
        "            #conved = [batch size, hid dim, trg sent len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "            \n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "         \n",
        "        #conved = [batch size, trg sent len, hid dim]\n",
        "            \n",
        "        output = self.out(self.dropout(conved))\n",
        "        \n",
        "        #output = [batch size, trg sent len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YKzR91APd7x3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "           \n",
        "        #calculate z^u (encoder_conved) and e (encoder_combined)\n",
        "        #encoder_conved is output from final encoder conv. block\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus positional embeddings \n",
        "        encoder_conved, encoder_combined = self.encoder(src)\n",
        "            \n",
        "        #encoder_conved = [batch size, src sent len, emb dim]\n",
        "        #encoder_combined = [batch size, src sent len, emb dim]\n",
        "        \n",
        "        #calculate predictions of next words\n",
        "        #output is a batch of predictions for each word in the trg sentence\n",
        "        #attention a batch of attention scores across the src sentence for each word in the trg sentence\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
        "        \n",
        "        #output = [batch size, trg sent len, output dim]\n",
        "        #attention = [batch size, trg sent len, src sent len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAOesGf0d7x7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_LAYERS = 10\n",
        "DEC_LAYERS = 10\n",
        "ENC_KERNEL_SIZE = 3\n",
        "DEC_KERNEL_SIZE = 3\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "    \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, PAD_IDX, device)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yeuazc9sd7yB",
        "colab_type": "code",
        "outputId": "5d5dbfe7-06cf-450e-97bb-7a48d02c6aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 37,811,973 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QOOEwj0_d7yK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XDxB-Qgqd7yQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JSnBIYsWsx4P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def score_the_translation(iterator, model):\n",
        "    \"\"\"\n",
        "    score_the_translation(test_iterator)\n",
        "    \"\"\"\n",
        "    BLEUscore = []\n",
        "    total_sentence = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        predicted = model(batch.src)\n",
        "        trg = batch.trg\n",
        "        for each_src_sent, each_trg_sent in zip(predicted,trg):\n",
        "            predicted = [SRC.vocab.itos[word] for word in each_src_sent]\n",
        "            original = [TRG.vocab.itos[word] for word in each_trg_sent]\n",
        "            BLEUscore.append(nltk.translate.bleu_score.sentence_bleu([predicted], original))\n",
        "            total_sentence += 1\n",
        "    return (sum(BLEUscore)/total_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BgKkWrdErl80",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_sentence(each_sent, root):\n",
        "    sentence = [root.vocab.itos[word] for word in each_sent]\n",
        "    return  sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5ekVYfWtOKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JIPTd9rEd7yW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "        #output = [batch size, trg sent len - 1, output dim]\n",
        "        #trg = [batch size, trg sent len]\n",
        "        \n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        #output = [batch size * trg sent len - 1, output dim]\n",
        "        #trg = [batch size * trg sent len - 1]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qz63Nu5yd7yf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            print(src.shape, trg[:,:-1].shape)\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "            #output = [batch size, trg sent len - 1, output dim]\n",
        "            #trg = [batch size, trg sent len]\n",
        "\n",
        "            output = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * trg sent len - 1, output dim]\n",
        "            #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "szTJ-R7Nd7yn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gM7inZfRd7yw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N_EPOCHS = 10\n",
        "# CLIP = 1\n",
        "# SAVE_DIR = 'models'\n",
        "# MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'tut5_model.pt')\n",
        "\n",
        "# best_valid_loss = float('inf')\n",
        "\n",
        "# if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "#     os.makedirs(f'{SAVE_DIR}')\n",
        "\n",
        "# for epoch in tqdm(range(N_EPOCHS)):\n",
        "    \n",
        "#     start_time = time.time()\n",
        "    \n",
        "#     train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "#     valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "#     end_time = time.time()\n",
        "    \n",
        "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    \n",
        "#     print(train_loss,  math.exp(train_loss), valid_loss, math.exp(valid_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2o-tlTXOg3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#backup code\n",
        "# def test_output():\n",
        "#     for i, batch in enumerate(test_iterator):\n",
        "#         all_output = []\n",
        "#         src = batch.src\n",
        "#         trg = batch.trg\n",
        "#         for i in range(0,len(src)):\n",
        "#             output, _ =  model(src, trg[:,:1]) #model(src[:,:1], trg[:,:1])\n",
        "#             decoded_batch  = to_sentence(torch.argmax(torch.softmax(output,dim=2),dim=2),TRG)\n",
        "            \n",
        "#             decoded_batch_stoi = torch.tensor([TRG.vocab.stoi[i] for i in decoded_batch]).type(torch.LongTensor).cuda()\n",
        "#             decoded_batch_stoi = decoded_batch_stoi.view(-1,1)\n",
        "#             trg = decoded_batch_stoi\n",
        "#             all_output.append(output)\n",
        "#         break\n",
        "#         all_output = np.array(all_output).T\n",
        "#         print(all_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zer2mA9ECTDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def test_output():\n",
        "#     for i, batch in enumerate(test_iterator):\n",
        "#         all_output = []\n",
        "#         src = batch.src\n",
        "#         trg = batch.trg\n",
        "#         for i in range(0,len(src)):\n",
        "#             output, _ =  model(src, trg[:,:1]) #model(src[:,:1], trg[:,:1])\n",
        "#             decoded_batch  = to_sentence(torch.argmax(torch.softmax(output,dim=2),dim=2),TRG)\n",
        "            \n",
        "#             decoded_batch_stoi = torch.tensor([TRG.vocab.stoi[i] for i in decoded_batch]).type(torch.LongTensor).cuda()\n",
        "#             decoded_batch_stoi = decoded_batch_stoi.view(-1,1)\n",
        "#             trg = decoded_batch_stoi\n",
        "#             all_output.append(output)\n",
        "#         break\n",
        "#         all_output = np.array(all_output).T\n",
        "#         print(all_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v_9GfMx6Lorz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cgqnxVAaF_J8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "    \n",
        "    simultaneous_genration = 3\n",
        "    final_output = torch.zeros(simultaneous_genration,30).type(torch.LongTensor).cuda()\n",
        "#     print(src.shape, trg.shape)\n",
        "#     print(to_sentence(src[0], SRC))\n",
        "#     print(to_sentence(trg[0], TRG))\n",
        "#     print( trg[:1,:].shape)\n",
        "    sos_tokens = np.array([ TRG.vocab.stoi['<sos>'] for i in range(simultaneous_genration)]).reshape(simultaneous_genration, 1)\n",
        "    final_output[:,:0] = torch.Tensor(sos_tokens).type(torch.LongTensor).cuda()\n",
        "#     print(\"trg : \", trg.shape)\n",
        "    for i in range(0,30):\n",
        "        print(src[:3,:].shape, final_output.shape)\n",
        "        output, _ = model(src[:3,:i], final_output)\n",
        "        predicted = torch.argmax(torch.softmax(output,dim=2),dim=2)\n",
        "#         predicted_batch_stoi = to_sentence(torch.argmax(torch.softmax(output,dim=2),dim=2), TRG)\n",
        "        print(\"predicted : \", predicted.shape)\n",
        "        final_output[:,:i] = predicted\n",
        "#         trg = final_output[:,:i].type(torch.LongTensor).cuda()\n",
        "#         final_output = np.array(final_output)\n",
        "        print(final_output)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xH4rEzEFd7y_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0c6PQE_2m_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score_the_translation(test_iterator,model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CftFN1t42qgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}