# How this blog series is different?
This is not just an ordinary blog a where I will show you some equation and sign off. This is an end to end blog series about Sequence to Sequence, We will start off with general Sequence to Sequence architecture, then one by one we will implement [attention](https://arxiv.org/abs/1706.03762), [beam search](https://guillaumegenthial.github.io/sequence-to-sequence.html), batch processing with attention, [Pointer networks](https://arxiv.org/abs/1506.03134). A series of the blog where algorithm, equations, and Implementation will be parallelly covered. 

**Sequence to Sequence is one size fit all kind of algorithm which is used in all below given tasks: 
**

- Machine Translation
- Summarization
- Question Answering
- chit chatbot
- Text Simplification
- image to code
- Speech to text
- text to speech


I am very sure you may find hundreads of blogs with thsese topics, but there are very few which are actually helps in learning, many f them are just showing equations as copied from original papers. many will just throw code and wont help in understanding why it was implemented, what equationas are governing the flow. Many will show you implementation but you wont be ablww to reproduce or use that implementation (bugs ?? errors ?? questionable extensibility..).
In present series we will be using PyTorch as is having intuitive model building approaches. It is easy understand and debug code with PyTorch.

http://libertine-fonts.org/








